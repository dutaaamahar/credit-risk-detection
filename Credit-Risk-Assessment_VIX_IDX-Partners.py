# -*- coding: utf-8 -*-
"""Credit_Risk_Prediction_v01_180324.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I49NluMz9lRFXRbGaq5WxqYok9cbaBib
"""

!pip install pycaret

import numpy as np
import pandas as pd
import seaborn as sns
import scipy.stats as stats
import matplotlib.pyplot as plt

from pycaret.classification import *

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from scipy.stats import ks_2samp

from sklearn.metrics import roc_auc_score
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, roc_auc_score
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import cross_val_predict, GridSearchCV, KFold, RandomizedSearchCV, train_test_split

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

pd.set_option('display.max_columns', None)

loan_data = pd.read_csv('/content/drive/MyDrive/Datasets/loan_data_2007_2014.csv')
print("Load loan dataset succeed!")

loan_data.drop(loan_data.columns[0], axis=1, inplace=True)
loan_data.head()

"""# Data Understanding"""

loan_data.info()

nums = []
cats = []

for feature in loan_data.columns:
    if loan_data[feature].dtype == 'object':
        cats.append(feature)
    else:
        nums.append(feature)

print("number of numerical features: ", len(nums))
print("number of categorical features: ", len(cats))

"""## Missing Value"""

null_col = [col for col in loan_data.columns if loan_data[col].isnull().all()]
print("non-null columns:")
print(null_col)
print("\nnumber of non-null columns:")
print(len(null_col), "columns")

print("dataframe shape before dropped columns: ", loan_data.shape)

df = loan_data.drop(columns=null_col, axis=1)

print("dataframe shape after dropped columns: ", df.shape)

df.info()

null_percentage = round((df.isnull().sum() * 100) / len(df), 2)
df_null = pd.DataFrame({'null_percentage': null_percentage})
df_null.sort_values('null_percentage', ascending=False, inplace=True)
df_null = df_null[df_null['null_percentage'] > 0]
df_null

"""## Duplicated Data"""

duplicated_data = df.duplicated().sum()

print("number of duplicated data:")
if duplicated_data > 0:
    print(duplicated_data, "data")
else:
    print("there is no duplicated data")

"""# Data Preparation

## Removing unused columns
"""

print("dataframe shape before dropped columns: ", df.shape)

df = df[df.columns[~df.columns.isin(['emp_title', 'url', 'desc', 'title', 'zip_code', 'addr_state', 'application_type'])]]

print("dataframe shape after dropped columns: ", df.shape)

"""## Handling Missing Values

### Removing `mths_since_last_record`, `mths_since_last_major_derog`, `mths_since_last_delinq` and `next_pymnt_d`
"""

print("dataframe shape before dropped columns: ", df.shape)

df = df[df.columns[~df.columns.isin(['mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq', 'next_pymnt_d'])]]

print("dataframe shape after dropped columns: ", df.shape)

"""### Imputation"""

def impute_missing(df):
    """
    Impute missing values in a DataFrame.

    Parameters:
    df (DataFrame): Input DataFrame with missing values.

    Returns:
    DataFrame: DataFrame with missing values imputed.
    """
    # Separate categorical and numerical columns
    cat_cols = df.select_dtypes(include=['object']).columns
    num_cols = df.select_dtypes(include=['number']).columns

    # Impute categorical columns with mode
    for col in cat_cols:
        df[col].fillna(df[col].mode()[0], inplace=True)

    # Impute numerical columns with median
    for col in num_cols:
        df[col].fillna(df[col].median(), inplace=True)

    return df

df = impute_missing(df)
print("Impute missing values succeed!")

df.isnull().sum()

"""## Checking Unique Values"""

print("number of unique values from numeric columns:")
print(df.select_dtypes(exclude='object').nunique())

"""

*   `id` and `member_id` have unique values ​​in each row
*   `policy_code` only have single unique value

"""

print("number of unique values from categorical columns:")
print(df.select_dtypes(include='object').nunique())

"""

*   `earliest_cr_line` has more than 500 unique values
*   `issue_d`, `last_pymnt_d`, and `last_credit_pull_d` have at least more than 50 unique values (below 500)
*   `term`, `grade`, `sub_grade`, `emp-length`, `home_ownership`, `verification_status`, `loan_status`, `pymnt_plan`, `purpose`, `initial_list_status` has less than 50 unique values

"""

df['term'].unique()

"""

*   Because there is a whitespace, I need to remove it

"""

df['term'] = df['term'].apply(lambda x: x.strip())
print("Removing whitespace succeed!")
df['term'].unique()

df['grade'].unique()

df['sub_grade'].unique()

df['emp_length'].unique()

df['home_ownership'].unique()

df['verification_status'].unique()

df['pymnt_plan'].unique()

df['purpose'].unique()

df['initial_list_status'].unique()

"""## Creating Target Variable

*   `target` feature would be our y variable
*   The `target` feature will be created by doing feature engineering from the `loan_status` feature, so the `loan_status` feature can be dropped later
"""

df['loan_status'].unique()

"""

*   Good Loan (1): `Fully Paid`, `Does not meet the credit policy. Status:Fully Paid`
*   Bad Loan (0): `Charged Off`, `Default`, `Late (31-120 days)`, `In Grace Period`, `Late (16-30 days)`, `Does not meet the credit policy. Status:Charged Off`
*   Does Not Meet Requirements for Both (-1): `Current`
*   The Good Loan (1) and the Bad Loan (0) later will be used for binary classification
*   Later, Undetachable Loan (-1) columns will be dropped because it is still current loan in progress that can not be detected as good or bad

"""

def create_target_variable(df, target_dict):
    """
    Create a new target variable based on the provided dictionary.

    Parameters:
    df (DataFrame): Input DataFrame containing the original status column.
    target_dict (dict): Dictionary mapping original status values to target values.

    Returns:
    DataFrame: DataFrame with the new target variable added.
    """
    # Create a new target variable based on the dictionary mapping
    df['target'] = df['loan_status'].map(target_dict)

    # Filter out rows where 'target' is not in [0, 1]
    df_filtered = df[df['target'].isin([0, 1])].reset_index(drop=True)

    return df_filtered

target_dict = {'Fully Paid': 1,
               'Does not meet the credit policy. Status:Fully Paid': 1,
               'Charged Off': 0,
               'Default': 0,
               'Late (31-120 days)': 0,
               'In Grace Period': 0,
               'Late (16-30 days)': 0,
               'Does not meet the credit policy. Status:Charged Off': 0,
               'Current': -1}

df = create_target_variable(df, target_dict)
print("Creating target variable succeed!")

df.head()

"""## Datetime Setting"""

datetime_col = df[['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d']]

for col in datetime_col:
    print(col)
    print(datetime_col[col].value_counts())
    print("-----" * 10)

# Set standard datetime
df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%y')
df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'], format='%b-%y')
df['last_credit_pull_d'] = pd.to_datetime(df['last_credit_pull_d'], format='%b-%y')
print("Formatting the datetime feature succeed!")

updated_datetime_col = df[['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d']]

for col in updated_datetime_col:
    print(col)
    print(updated_datetime_col[col].value_counts())
    print("-----" * 10)

"""

*   Because there is a year greater than 2016 on the earliest_cr_line feature, I need to adjust it by subtracting the year by 100

"""

def date_time(dt):
    """
    Adjusts the year component of a datetime object if it's greater than 2016.

    Parameters:
    dt (datetime): Input datetime object.

    Returns:
    datetime: Adjusted datetime object.
    """
    # Check if the year is greater than 2016
    if dt.year > 2016:
        # If yes, subtract 100 years from the year component
        dt = dt.replace(year=dt.year - 100)

    # Return the adjusted datetime object
    return dt

df['earliest_cr_line'] = df['earliest_cr_line'].apply(lambda x: date_time(x))
print("Adjusting the datetime feature succeed!")

df.head()

df.info()

df.to_csv('cleaned_loan_data_2007_2014.csv', index=False)
!cp 'cleaned_loan_data_2007_2014.csv' '/content/drive/MyDrive/Datasets'
print("Saving cleaned data succeed!")

"""*   This cleaned dataset will be used for analyzing in Exploratory Data Analysis (EDA) to extract insight

# Exploratory Data Analysis
"""

df = pd.read_csv("/content/drive/MyDrive/Datasets/cleaned_loan_data_2007_2014.csv")
print("Load cleaned data succeed!")

df.head()

"""## Analyzing Descriptive Statistics

### Numerical Features
"""

df.describe()

"""### Categorical Features"""

df.describe(exclude=['int', 'float'])

"""## Univariate Analysis

### Categorical Features
"""

term = df.groupby('term').agg({'id': 'count'}).reset_index()
term.columns=['term', 'total']
term.sort_values(by='total', ascending=False, inplace=True)
term.set_index('term', inplace=True)
term['percentage'] = round((term['total'] * 100) / sum(term['total']), 2)
term

plt.figure(figsize=(5,5))
sns.countplot(data=df, x='term', hue='term')
plt.title('Distribution of Terms')
plt.xlabel('Term')
plt.ylabel('Count')
plt.show()

"""

*   Borrowers overwhelmingly favor 36-month terms (77.85%) over 60-month terms (22.15%).
*   Shorter terms may be seen as less risky by lenders, possibly due to quicker repayment.

"""

grade = df.groupby('grade').agg({'id': 'count'}).reset_index()
grade.columns=['grade', 'total']
grade.sort_values(by='total', ascending=False, inplace=True)
grade.set_index('grade', inplace=True)
grade['percentage'] = round((grade['total'] * 100) / sum(grade['total']), 2)
grade

plt.figure(figsize=(5,5))
sns.barplot(data=grade, x='grade', y='total', hue='grade')
plt.title('Distribution of Grades')
plt.xlabel('Grade')
plt.ylabel('Count')
plt.show()

"""

*   Grades B and C dominate, comprising over half (55.71%) of loans.
*   Grades A, B, and C represent about 72% of loans, suggesting a moderate risk profile.
*   Grades D-G make up around 28% of loans, indicating fewer high-risk borrowers.

"""

sub_grade = df.groupby('sub_grade').agg({'id': 'count'}).reset_index()
sub_grade.columns=['sub_grade', 'total']
sub_grade.sort_values(by='total', ascending=False, inplace=True)
sub_grade.set_index('sub_grade', inplace=True)
sub_grade['percentage'] = round((sub_grade['total'] * 100) / sum(sub_grade['total']), 2)
sub_grade

# Filter top 5 subgrades
top_5_subgrades = sub_grade.head(5)

# Plotting the bar plot for top 5 subgrades
plt.figure(figsize=(5, 5))
sns.barplot(data=top_5_subgrades, x='sub_grade', y='total', hue='sub_grade')
plt.title('Distribution of Top 5 Sub Grades')
plt.xlabel('Sub Grade')
plt.ylabel('Count')
plt.show()

"""

*   The top 5 sub-grades are all within grade B, comprising about 30.23% of loans.
*   Different sub-grades within grade B suggest varying creditworthiness levels.
*   Lenders should consider nuances within grade B when assessing risk and making lending decisions.

"""

emp_length = df.groupby('emp_length').agg({'id': 'count'}).reset_index()
emp_length.columns=['emp_length', 'total']
emp_length.sort_values(by='total', ascending=False, inplace=True)
emp_length.set_index('emp_length', inplace=True)
emp_length['percentage'] = round((emp_length['total'] * 100) / sum(emp_length['total']), 2)
emp_length

plt.figure(figsize=(10,5))
sns.barplot(data=emp_length, x='emp_length', y='total', hue='emp_length')
plt.title('Distribution of Employment Length')
plt.xlabel('Employment Length')
plt.ylabel('Count')
plt.show()

"""

*   Majority of borrowers (33.72%) have over 10 years of employment.
*   Longer employment lengths suggest borrower stability.
*   Borrowers with shorter employment lengths may pose higher risk.
*   Lenders should consider employment length in assessing borrower stability and risk.

"""

home_ownership = df.groupby('home_ownership').agg({'id': 'count'}).reset_index()
home_ownership.columns=['home_ownership', 'total']
home_ownership.sort_values(by='total', ascending=False, inplace=True)
home_ownership.set_index('home_ownership', inplace=True)
home_ownership['percentage'] = round((home_ownership['total'] * 100) / sum(home_ownership['total']), 2)
home_ownership

plt.figure(figsize=(5,5))
sns.barplot(data=home_ownership, x='home_ownership', y='total', hue='home_ownership')
plt.title('Distribution of Home Ownership')
plt.xlabel('Home Ownership')
plt.ylabel('Count')
plt.show()

"""

*   Majority of borrowers (91.9%) either have a mortgage (49.09%) or rent (42.37%) their homes.
*   Mortgage holders indicate stable housing, while renters comprise a significant portion, suggesting a trend or barriers to homeownership.

"""

verification_status = df.groupby('verification_status').agg({'id': 'count'}).reset_index()
verification_status.columns=['verification_status', 'total']
verification_status.sort_values(by='total', ascending=False, inplace=True)
verification_status.set_index('verification_status', inplace=True)
verification_status['percentage'] = round((verification_status['total'] * 100) / sum(verification_status['total']), 2)
verification_status

plt.figure(figsize=(5,5))
sns.barplot(data=verification_status, x='verification_status', y='total', hue='verification_status')
plt.title('Distribution of Verification Status')
plt.xlabel('Verification Status')
plt.ylabel('Count')
plt.show()

"""

*   Borrowers are divided into Verified (37.19%), Not Verified (34.62%), and Source Verified (28.19%).
*   Majority of borrowers (65.38%) undergo some form of verification, indicating trustworthiness. Verified borrowers may be perceived as lower risk.
*   Verification is crucial in the lending process for assessing borrower credibility and managing risk.

"""

loan_status = df.groupby('loan_status').agg({'id': 'count'}).reset_index()
loan_status.columns=['loan_status', 'total']
loan_status.sort_values(by='total', ascending=False, inplace=True)
loan_status.set_index('loan_status', inplace=True)
loan_status['percentage'] = round((loan_status['total'] * 100) / sum(loan_status['total']), 2)
loan_status

plt.figure(figsize=(5,5))
sns.barplot(data=loan_status, x='loan_status', y='total', hue='loan_status')
plt.title('Distribution of Loan Status')
plt.xlabel('Loan Status')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

"""

*   Most loans are fully paid (76.32%), followed by charged off loans (17.55%).
*   Charged off and default loans indicate higher risk, while fully paid loans reflect successful repayment.
*   Lenders need to closely watch late payments and non-compliant loans to manage risk and ensure policy compliance.

"""

pymnt_plan = df.replace({'pymnt_plan': {'y': 'Yes', 'n': 'No'}}).groupby('pymnt_plan').agg({'id': 'count'}).reset_index()
pymnt_plan.columns=['pymnt_plan', 'total']
pymnt_plan.sort_values(by='total', ascending=False, inplace=True)
pymnt_plan.set_index('pymnt_plan', inplace=True)
pymnt_plan['percentage'] = round((pymnt_plan['total'] * 100) / sum(pymnt_plan['total']), 2)
pymnt_plan

plt.figure(figsize=(5,5))
sns.barplot(data=pymnt_plan, x='pymnt_plan', y='total', hue='pymnt_plan')
plt.title('Distribution of Payment Plan')
plt.xlabel('Payment Plan')
plt.ylabel('Count')
plt.show()

"""

*   Nearly all loans (100%) don't have a payment plan, with only a tiny fraction (0.0%) opting for one.
*   Borrowers generally don't utilize payment plans, preferring standard repayment terms.
*   The rarity of payment plans suggests they have little impact on overall loan dynamics.

"""

purpose = df.groupby('purpose').agg({'id': 'count'}).reset_index()
purpose.columns=['purpose', 'total']
purpose.sort_values(by='total', ascending=False, inplace=True)
purpose.set_index('purpose', inplace=True)
purpose['percentage'] = round((purpose['total'] * 100) / sum(purpose['total']), 2)
purpose

plt.figure(figsize=(10,5))
sns.barplot(data=purpose, x='purpose', y='total', hue='purpose')
plt.title('Distribution of Purpose')
plt.xlabel('Purpose')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

"""

*   Majority of loans are for debt consolidation (58.28%) and credit card payments (19.94%).
*   Other significant purposes include home improvement (5.82%) and major purchases (2.49%).
*   Loans for small businesses, cars, medical expenses, weddings, and moving are less common, each comprising less than 2.5% of the total.
*   Loans for educational expenses, renewable energy, vacations, and housing are rare, each representing less than 1% of the total.
*   The prevalence of debt consolidation and credit card payments suggests that borrowers commonly use loans to manage existing debts or make large purchases.

"""

initial_list_status = df.replace({'initial_list_status': {'w': 'Whole', 'f': 'Fraction'}}).groupby('initial_list_status').agg({'id': 'count'}).reset_index()
initial_list_status.columns=['initial_list_status', 'total']
initial_list_status.sort_values(by='total', ascending=False, inplace=True)
initial_list_status.set_index('initial_list_status', inplace=True)
initial_list_status['percentage'] = round((initial_list_status['total'] * 100) / sum(initial_list_status['total']), 2)
initial_list_status

plt.figure(figsize=(5,5))
sns.barplot(data=initial_list_status, x='initial_list_status', y='total', hue='initial_list_status')
plt.title('Distribution of Initial List Status')
plt.xlabel('Initial List Status')
plt.ylabel('Count')
plt.show()

"""

*   Majority of loans (74.18%) are listed with fractional ownership, while 25.82% are listed as whole loans.
*   Fractional listings are more common, suggesting investor interest in diversification through partial loan investments.

"""

target = df.replace({'target': {0: 'Bad Loan', 1: 'Good Loan'}}).groupby('target').agg({'id': 'count'}).reset_index()
target.columns=['target', 'total']
target.sort_values(by='total', ascending=False, inplace=True)
target.set_index('target', inplace=True)
target['percentage'] = round((target['total'] * 100) / sum(target['total']), 2)
target

plt.figure(figsize=(5,2.5))
sns.barplot(data=target, x='target', y='total', hue='target')
plt.title('Distribution of Credit Risk')
plt.xlabel('Credit Risk')
plt.ylabel('Count')
plt.show()

"""### Numerical Features"""

non_used = ['id', 'member_id', 'target']
uni_dist = df.select_dtypes(include=[np.float64,np.int64])
uni_dist = uni_dist[uni_dist.columns[~uni_dist.columns.isin(non_used)]]

plt.figure(figsize=(30, 30))
for col in range(len(uni_dist.columns)):
    plt.subplot(10, 3, col+1)
    sns.histplot(x=uni_dist.iloc[:, col], color='grey')
    plt.xlabel(uni_dist.columns[col])
plt.tight_layout()
plt.show()

plt.figure(figsize=(30, 30))
for col in range(len(uni_dist.columns)):
    plt.subplot(10, 3, col+1)
    sns.boxplot(x=uni_dist.iloc[:, col], color='grey')
    plt.xlabel(uni_dist.columns[col])
plt.tight_layout()
plt.show()

"""## Bivariate Analysis"""

loan_purpose = df.groupby(['purpose', 'target']).size().reset_index(name='total')
loan_purpose = loan_purpose.pivot(index='purpose', columns='target', values='total').fillna(0)
loan_purpose['total'] = loan_purpose.sum(axis=1)
loan_purpose.sort_values(by='total', ascending=False, inplace=True)
loan_purpose = loan_purpose.drop('total', axis=1)

target_dict = {'debt_consolidation': 'DEBT CONSOLIDATION',
               'credit_card': 'CREDIT CARD',
               'home_improvement': 'HOME IMPROVEMENT',
               'other': 'OTHER',
               'major_purchase': 'MAJOR PURCHASE',
               'small_business': 'SMALL BUSINESS',
               'car': 'CAR',
               'medical': 'MEDICAL',
               'wedding': 'WEDDING',
               'moving': 'MOVING',
               'house': 'HOUSE',
               'vacation': 'VACATION',
               'educational': 'EDUCATIONAL',
               'renewable_energy': 'RENEWABLE ENERGY'}
loan_purpose.rename(index=target_dict, inplace=True)

target_dict = {0: 'Bad Loan',
               1: 'Good Loan'}
loan_purpose.rename(columns=target_dict, inplace=True)
loan_purpose = loan_purpose[['Good Loan', 'Bad Loan']]
loan_purpose

top_5_loan_purpose = loan_purpose.head()

plt.figure(figsize=(10, 10))
top_5_loan_purpose.plot(kind='barh', stacked=True, color=['blue', 'red'])
plt.xlabel("Count")
plt.ylabel("Purpose")
plt.title("The Reason Why Customers Take Credit Loans Based On Credit Risk")
plt.show()

loan_term = df.groupby(['term', 'target']).size().reset_index(name='total')
loan_term = loan_term.pivot(index='term', columns='target', values='total').fillna(0)
loan_term['total'] = loan_term.sum(axis=1)
loan_term.sort_values(by='total', ascending=False, inplace=True)
loan_term = loan_term.drop('total', axis=1)

target_dict = {0: 'Bad Loan',
               1: 'Good Loan'}
loan_term.rename(columns=target_dict, inplace=True)
loan_term = loan_term[['Good Loan', 'Bad Loan']]
loan_term

plt.figure(figsize=(10, 10))
loan_term.plot(kind='bar', stacked=True, color=['blue', 'red'])
plt.title("The Length of Time a Customer Takes a Credit Loan Based On Credit Risk")
plt.xlabel("Term")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.show()

"""## Multivariate Analysis"""

fig = plt.figure(figsize=(30, 30))
sns.heatmap(uni_dist.corr(), cmap='Blues', annot=True);

"""# Data Preprocessing

## Feature Selection
"""

df = pd.read_csv("/content/drive/MyDrive/Datasets/cleaned_loan_data_2007_2014.csv")
print("Load cleaned data succeed!")

df.info()

unused_col = ['id', 'member_id', 'issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d', 'policy_code']
multi_col = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'sub_grade', 'loan_status', 'revol_bal', 'total_acc',
             'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',
             'recoveries']
removed_col = unused_col + multi_col

df_selected = df[df.columns[~df.columns.isin(removed_col)]].reset_index(drop=True)
df_selected.head()

df_selected.to_csv('selected_loan_data_2007_2014.csv', index=False)
!cp 'selected_loan_data_2007_2014.csv' '/content/drive/MyDrive/Datasets'
print("Saving selected data succeed!")

"""## Feature Encoding"""

df_loan = pd.read_csv('/content/drive/MyDrive/Datasets/selected_loan_data_2007_2014.csv')
print("Load selected data succeed!")
df_loan.head()

df_loan.info()

df_loan['term'].unique()

df_loan['term'] = df_loan['term'].apply(lambda x: [int(text) for text in x.split() if text.isdigit()][0])
df_loan.head()

df_loan['grade'].unique()

target_dict = {'A': 6,
               'B': 5,
               'C': 4,
               'D': 3,
               'E': 2,
               'F': 1,
               'G': 0}
df_loan["grade"] = df_loan["grade"].map(target_dict)

df_loan.head()

df_loan['emp_length'].unique()

target_dict = {'< 1 year': 0,
               '1 year': 1,
               '2 years': 2,
               '3 years': 3,
               '4 years': 4,
               '5 years': 5,
               '6 years': 6,
               '7 years': 7,
               '8 years': 8,
               '9 years': 9,
               '10+ years': 10}
df_loan["emp_length"] = df_loan["emp_length"].map(target_dict)

df_loan.head()

df_loan['home_ownership'].unique()

df_loan['home_ownership'].value_counts()

"""

*   `ANY`, `NONE` will be aggregated into `OTHER`

"""

target_dict = {'MORTGAGE': 'MORTGAGE',
               'RENT': 'RENT',
               'OWN': 'OWN',
               'OTHER': 'OTHER',
               'ANY': 'OTHER',
               'NONE': 'OTHER'}
df_loan["home_ownership"] = df_loan["home_ownership"].map(target_dict)

df_loan.head()

encoder = OneHotEncoder(sparse=False)
home_ownership_enc = pd.DataFrame(encoder.fit_transform(df_loan[['home_ownership']]))
home_ownership_enc.columns = encoder.get_feature_names_out(['home_ownership'])
df_loan = pd.concat([df_loan, home_ownership_enc], axis=1)
df_loan.drop(["home_ownership"], axis=1, inplace=True)
df_loan.head()

df_loan['verification_status'].unique()

df_loan['verification_status'].value_counts()

target_dict = {'Verified': 'VERIFIED',
               'Not Verified': 'NOT VERIFIED',
               'Source Verified': 'SOURCE VERIFIED'}
df_loan['verification_status'] = df_loan['verification_status'].map(target_dict)

verification_status_enc = pd.DataFrame(encoder.fit_transform(df_loan[['verification_status']]))
verification_status_enc.columns = encoder.get_feature_names_out(['verification_status'])
df_loan = pd.concat([df_loan, verification_status_enc], axis=1)
df_loan.drop(["verification_status"], axis=1, inplace=True)
df_loan.head()

df_loan['pymnt_plan'].unique()

target_dict = {'n': 0,
               'y': 1}
df_loan['pymnt_plan'] = df_loan['pymnt_plan'].map(target_dict)

df_loan.head()

df_loan['purpose'].unique()

df_loan['purpose'].value_counts()

target_dict = {'debt_consolidation': 'DEBT CONSOLIDATION',
               'credit_card': 'CREDIT CARD',
               'home_improvement': 'HOME IMPROVEMENT',
               'other': 'OTHER',
               'major_purchase': 'MAJOR PURCHASE',
               'small_business': 'SMALL BUSINESS',
               'car': 'CAR',
               'medical': 'MEDICAL',
               'wedding': 'WEDDING',
               'moving': 'MOVING',
               'house': 'HOUSE',
               'vacation': 'VACATION',
               'educational': 'EDUCATIONAL',
               'renewable_energy': 'RENEWABLE ENERGY'}
df_loan['purpose'] = df_loan['purpose'].map(target_dict)

df_loan['purpose'].value_counts()

"""

*   `HOME IMPROVEMENT`, `CAR`, `MEDICAL`, `WEDDING`, `MOVING`, `HOUSE`, `VACATION`, `EDUCATIONAL` will be aggregated into `PRIVATE USE`
*   `RENEWABLE ENERGY` will be aggregated into `OTHER`

"""

target_dict = {'DEBT CONSOLIDATION': 'DEBT CONSOLIDATION',
               'CREDIT CARD': 'CREDIT CARD',
               'HOME IMPROVEMENT': 'PRIVATE USE',
               'OTHER': 'OTHER',
               'MAJOR PURCHASE': 'MAJOR PURCHASE',
               'SMALL BUSINESS': 'SMALL BUSINESS',
               'CAR': 'PRIVATE USE',
               'MEDICAL': 'PRIVATE USE',
               'WEDDING': 'PRIVATE USE',
               'MOVING': 'PRIVATE USE',
               'HOUSE': 'PRIVATE USE',
               'VACATION': 'PRIVATE USE',
               'EDUCATIONAL': 'PRIVATE USE',
               'RENEWABLE ENERGY': 'OTHER'}
df_loan['purpose'] = df_loan['purpose'].map(target_dict)

df_loan['purpose'].value_counts()

purpose_enc = pd.DataFrame(encoder.fit_transform(df_loan[['purpose']]))
purpose_enc.columns = encoder.get_feature_names_out(['purpose'])
df_loan = pd.concat([df_loan, purpose_enc], axis=1)
df_loan.drop(["purpose"], axis=1, inplace=True)
df_loan.head()

df_loan['initial_list_status'].unique()

target_dict = {'f': 'FRACTIONAL',
               'w': 'WHOLE'}
df_loan['initial_list_status'] = df_loan['initial_list_status'].map(target_dict)

initial_list_status_enc = pd.DataFrame(encoder.fit_transform(df_loan[['initial_list_status']]))
initial_list_status_enc.columns = encoder.get_feature_names_out(['initial_list_status'])
df_loan = pd.concat([df_loan, initial_list_status_enc], axis=1)
df_loan.drop(["initial_list_status"], axis=1, inplace=True)
df_loan.head()

df_loan.info()

df_loan.to_csv('enc_loan_data_2007_2014.csv', index=False)
!cp 'enc_loan_data_2007_2014.csv' '/content/drive/MyDrive/Datasets'
print("Saving encoding data succeed!")

"""## Handling Outliers"""

df_loan.head()

def remove_outliers_iqr(df, columns, threshold=1.5):
    """
    Remove outliers from a DataFrame using the Interquartile Range (IQR) method.

    Parameters:
        df (DataFrame): Input DataFrame.
        columns (list): List of columns to consider for outlier removal.
        threshold (int or float): Threshold value for outlier detection.
            Values beyond Q3 + threshold * IQR or below Q1 - threshold * IQR
            are considered outliers. Default is 1.5.

    Returns:
        DataFrame: DataFrame with outliers removed.
    """
    df_out = df.copy()
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR

        df_out = df_out[(df_out[col] >= lower_bound) & (df_out[col] <= upper_bound)]
    return df_out

outlier_col = ['installment', 'annual_inc', 'open_acc', 'total_rec_late_fee', 'collection_recovery_fee',
               'last_pymnt_amnt', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']

print("dataframe shape before removing outliers: ", df_loan.shape)

for col in outlier_col:
    df_out = remove_outliers_iqr(df_loan, [col])

print("dataframe shape after removing outliers: ", df_out.shape)

df_out.to_csv('out_loan_data_2007_2014.csv', index=False)
!cp 'out_loan_data_2007_2014.csv' '/content/drive/MyDrive/Datasets'
print("Saving non-outliers data succeed!")

"""# Modelling"""

df_loan = pd.read_csv('/content/drive/MyDrive/Datasets/out_loan_data_2007_2014.csv')
print("Load prepared data succeed!")

df_loan.head()

df_loan.info()

"""## Split Train Test Data"""

X = df_loan.loc[:, df_loan.columns != 'target']
y = df_loan['target']

def split_data(X, y, test_size=0.2, random_state=42):
    """
    Split the data into train, validation, and test sets.

    Parameters:
        X (array-like): The features.
        y (array-like): The target variable.
        test_size (float, optional): The proportion of the dataset to include in the test split. Defaults to 0.2.
        random_state (int, optional): Controls the shuffling applied to the data before applying the split. Defaults to 42.

    Returns:
        tuple: Four arrays: X_train, X_val, y_train, y_val
    """
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, shuffle=True, random_state=random_state)

    X_test = X_val.iloc[-1, :]
    y_test = y_val.iloc[-1]
    y_test = np.array(y_test).reshape(1)
    X_val = X_val.iloc[:-1, :]
    y_val = y_val.iloc[:-1]

    return X_train, X_val, y_train, y_val, X_test, y_test

X_train, X_val, y_train, y_val, X_test, y_test = split_data(X, y)
print("Splitting data succeed!")

from collections import Counter
print("number of classes from training data: ", Counter(y_train))
print("number of classes from validation data: ", Counter(y_val))

"""## Find the Top 3 Best Algorithms using PyCaret"""

train = pd.concat([X_train, y_train], axis=1)
validation = pd.concat([X_val, y_val], axis=1)

# init setup
reg = setup(data=train, target='target')

best_model = compare_models(sort='Accuracy')

"""## Transform Pipeline"""

def log_transform(x):
    return np.log(x+1)

# Pipeline to transform the numerical features
numerical = ['int_rate', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc',
             'revol_util', 'total_rec_late_fee', 'collection_recovery_fee', 'last_pymnt_amnt', 'tot_coll_amt',
             'tot_cur_bal', 'total_rev_hi_lim']
skewed = ['installment', 'annual_inc', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'revol_util',
          'total_rec_late_fee', 'collection_recovery_fee', 'last_pymnt_amnt', 'tot_coll_amt', 'tot_cur_bal',
          'total_rev_hi_lim']
diff = list(set(numerical) - set(skewed))

smt = SMOTE(random_state=42)
ss = StandardScaler()
log_transform = FunctionTransformer(log_transform)

numerical_transformer = Pipeline([
    ('log_transform', log_transform),
     ('scaler', ss)])

ct = ColumnTransformer([
    ('num_transform', numerical_transformer, skewed),
     ('scaler', ss, diff)], remainder='passthrough')

def evaluate_ks_and_roc_auc(y_real, y_proba):
    """
    Evaluate Kolmogorov-Smirnov (KS) statistic and ROC AUC.

    Parameters:
        y_real (array-like): Ground truth labels.
        y_proba (array-like): Predicted probabilities for the positive class.

    Returns:
        tuple: KS statistic and ROC AUC score.
    """
    # Unite both visions to be able to filter
    df = pd.DataFrame({'real': y_real, 'proba': y_proba[:, 1]})

    # Recover each class
    class0 = df[df['real'] == 0]
    class1 = df[df['real'] == 1]

    # Calculate KS statistic
    ks = ks_2samp(class0['proba'], class1['proba'])

    # Calculate ROC AUC score
    roc_auc = roc_auc_score(df['real'], df['proba'])

    print(f"ROC AUC: {roc_auc:.4f}")
    print(f"KS: {ks.statistic:.4f} (p-value: {ks.pvalue:.3e})")

    return ks.statistic, roc_auc

def plot_confusion_matrix(cm,
                          target_names,
                          title="Confusion Matrix",
                          cmap=None,
                          normalize=True):
    """
    Plot a confusion matrix.

    Parameters:
        cm (array-like): Confusion matrix.
        target_names (list): Class names.
        title (str, optional): Title of the plot. Defaults to 'Confusion matrix'.
        cmap (matplotlib colormap, optional): The colormap to use. Defaults to None.
        normalize (bool, optional): If True, plot the proportions. If False, plot the raw numbers. Defaults to True.

    Returns:
        None
    """
    import itertools

    accuracy = np.trace(cm) / np.sum(cm).astype('float')
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel("True label")
    plt.xlabel("Predicted label\naccuracy={:0.4f}; misclass={:0.4f}".format(accuracy, misclass))
    plt.grid(False)
    plt.show()

"""## Extreme Gradient Boosting"""

# Create an instance of XGBClassifier
xgb = XGBClassifier(random_state=42)

# Main pipeline for fitting.
xgb_model = Pipeline([
    ('col_transform', ct),
     ('smt', smt),
      ('xgb', xgb)])

xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict_proba(X_val)
predicted = xgb_model.predict(X_val)

# Print AUC, KS Score, and Classification Report
ks, auc = evaluate_ks_and_roc_auc(y_val, y_pred_xgb)
matrix = classification_report(y_val, predicted)
print("Classification Report XGBoost Classifier: \n", matrix)
cm = confusion_matrix(y_val, predicted)
target_names = ['Bad Loan', 'Good Loan']
plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=None, normalize=False)

"""## Light Gradient Boosting Machine"""

# Create an instance of LGBMClassifier
lgbm = LGBMClassifier(random_state=42)

# Main pipeline for fitting.
lgbm_model = Pipeline([
    ('col_transform', ct),
     ('smt', smt),
      ('lgbm', lgbm)])

lgbm_model.fit(X_train, y_train)
y_pred_lgbm = lgbm_model.predict_proba(X_val)
predicted = lgbm_model.predict(X_val)

# Print AUC, KS Score, and Classification Report
ks, auc = evaluate_ks_and_roc_auc(y_val, y_pred_lgbm)
matrix = classification_report(y_val, predicted)
print("Classification Report LightGBM Classifier: \n", matrix)
cm = confusion_matrix(y_val, predicted)
target_names = ['Bad Loan', 'Good Loan']
plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=None, normalize=False)

"""## Random Forest Classifier"""

# Create an instance of RandomForestClassifier
rf = RandomForestClassifier(random_state=42)

# Main pipeline for fitting.
rf_model = Pipeline([
    ('col_transform', ct),
     ('smt', smt),
      ('rf', rf)])

rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict_proba(X_val)
predicted = rf_model.predict(X_val)

# Print AUC, KS Score, and Classification Report
ks, auc = evaluate_ks_and_roc_auc(y_val, y_pred_rf)
matrix = classification_report(y_val, predicted)
print("Classification report Random Forest Classifier: \n", matrix)
cm = confusion_matrix(y_val, predicted)
target_names = ['Bad Loan', 'Good Loan']
plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=None, normalize=False)

"""## Compare Model Performance"""

# Initialize lists to store evaluation results
models = ['XGBClassifier', 'LGBMClassifier', 'RandomForestClassifier']
metrics = ['FN', 'Recall', 'ROC-AUC', 'KS']
results = []

# Evaluate each model
for model, y_proba in zip(models, [y_pred_xgb, y_pred_lgbm, y_pred_rf]):
    y_pred = np.argmax(y_proba, axis=1)

    cm = confusion_matrix(y_val, y_pred)

    fn = cm[1, 0]
    recall = round(recall_score(y_val, y_pred), 4)
    auc = round(evaluate_ks_and_roc_auc(y_val, y_proba)[1], 4)
    ks = round(evaluate_ks_and_roc_auc(y_val, y_proba)[0], 4)

    results.append([fn, recall, auc, ks])

# Create a DataFrame to store the results
model_performance = pd.DataFrame(results, columns=metrics, index=models)

model_performance

model_performance.to_csv('model_performance.csv', index=True)
!cp 'model_performance.csv' '/content/drive/MyDrive/Datasets'
print("Saving model performance data succeed!")

"""# Model Optimization & Evaluation

## Hyperparameter Tuning
"""

# Define the hyperparameter distributions
params = {
    'max_depth': stats.randint(3, 10),
    'learning_rate': stats.uniform(0.5, 0.1),
    'subsample': stats.uniform(0.1, 0.9),
    'n_estimators':stats.randint(50, 500)
}

# Create an instance of XGBClassifier
xgb = XGBClassifier(random_state=42)

# Main pipeline for fitting.
xgb_model_HT = Pipeline([
    ('col_transform', ct),
     ('smt', smt),
      ('xgb_HT', RandomizedSearchCV(xgb, params, n_iter=10, cv=7, scoring='accuracy'))])

xgb_model_HT.fit(X_train, y_train)
y_pred_xgb = xgb_model_HT.predict_proba(X_val)
predicted = xgb_model_HT.predict(X_val)

# Print AUC, KS Score, and Classification Report
ks, auc = evaluate_ks_and_roc_auc(y_val, y_pred_xgb)
matrix = classification_report(y_val, predicted)
print("Classification Report XGBoost Classifier: \n", matrix)
cm = confusion_matrix(y_val, predicted)
target_names = ['Bad Loan', 'Good Loan']
plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=None, normalize=False)

xgb_model_HT.steps[2][1].best_params_

# Create an instance of XGBClassifier
xgb = XGBClassifier(learning_rate=0.5289063031828417, max_depth=5,
                    n_estimators=448, subsample=0.9927436309213458,
                    random_state=42)

# Main pipeline for fitting.
xgb_model = Pipeline([
    ('col_transform', ct),
     ('smt', smt),
      ('xgb', xgb)])

xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict_proba(X_val)
predicted = xgb_model.predict(X_val)

# Print AUC, KS Score, and Classification Report
ks, auc = evaluate_ks_and_roc_auc(y_val, y_pred_xgb)
matrix = classification_report(y_val, predicted)
print("Classification report Logistic Regression: \n", matrix)
cm = confusion_matrix(y_val, predicted)
target_names = ['Bad Loan', 'Good Loan']
plot_confusion_matrix(cm, target_names, title='Confusion Matrix', cmap=None, normalize=False)

"""# Model Interpretation"""

feature_importance = pd.DataFrame()
feature_importance['features'] = X_train.columns
feature_importance['importance'] = xgb_model.named_steps['xgb'].feature_importances_

plt.figure(figsize=(30,10))
plot = feature_importance.sort_values('importance', ascending=False).head(10).plot.barh(color='blue', legend=None)
plot.set_yticklabels(feature_importance.sort_values('importance', ascending=False).head(10).features)
plt.title("Feature Importance in XGBoost Model")
plt.xlim([0,0.7])
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.gca().invert_yaxis()
plt.show()

"""Top 3 features importance in predicting credit risk (Good or Bad):

*   dti
*   initial_status_list_WHOLE
*   delinq_2yrs


"""